{{- define "elasticsearch.probe" -}}
exec:
  command:
    - bash
    - -c
    - |
      set -e

      # If the node is starting up wait for the cluster to be ready (request params: "{{ .Values.clusterHealthCheckParams }}" )
      # Once it has started only check that the node itself is responding
      START_FILE=/tmp/.es_start_file

      # Disable nss cache to avoid filling dentry cache when calling curl
      # This is required with Elasticsearch Docker using nss < 3.52
      export NSS_SDB_USE_CACHE=no

      http () {
        local path="${1}"
        local args="${2}"
        set -- -XGET -s

        if [ "$args" != "" ]; then
          set -- "$@" $args
        fi

        if [ ! -z ${ELASTIC_PASSWORD+x} ]; then
          set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
        fi

        curl --output /dev/null -k "$@" "{{ .Values.protocol }}://127.0.0.1:{{ .Values.httpPort }}${path}"
      }

      if [ -f "${START_FILE}" ]; then
        echo 'Elasticsearch is already running, lets check the node is healthy'
        HTTP_CODE=$(http "/" "-w %{http_code}")
        RC=$?
        if [[ ${RC} -ne 0 ]]; then
          echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} {{ .Values.protocol }}://127.0.0.1:{{ .Values.httpPort }}/ failed with RC ${RC}"
          exit ${RC}
        fi
        # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
        if [[ ${HTTP_CODE} == "200" ]]; then
          exit 0
        elif [[ ${HTTP_CODE} == "503" && "{{ include "elasticsearch.esMajorVersion" . }}" == "6" ]]; then
          exit 0
        else
          echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} {{ .Values.protocol }}://127.0.0.1:{{ .Values.httpPort }}/ failed with HTTP code ${HTTP_CODE}"
          exit 1
        fi

      else
        echo 'Waiting for elasticsearch cluster to become ready (request params: "{{ .Values.clusterHealthCheckParams }}" )'
        if http "/_cluster/health?{{ .Values.clusterHealthCheckParams }}" "--fail" ; then
          touch ${START_FILE}
          exit 0
        else
          echo 'Cluster is not yet ready (request params: "{{ .Values.clusterHealthCheckParams }}" )'
          exit 1
        fi
      fi
{{- end -}}
---
{{- $sizingReplicas := include "common.sizing.elasticsearch.replicas" . | trim -}}
{{- $replicas := int (default .Values.replicas $sizingReplicas) }}
apiVersion: {{ template "elasticsearch.statefulset.apiVersion" . }}
kind: StatefulSet
metadata:
  name: {{ template "elasticsearch.uname" . }}
  labels:
    {{- include "elasticsearch.labels.recommended" . | nindent 4 }}
    {{- include "elasticsearch.commonLabels" . | nindent 4 }}
    app.kubernetes.io/component: {{ .Values.nodeGroup }}
    {{- range $key, $value := .Values.labels }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
  annotations:
    esMajorVersion: "{{ include "elasticsearch.esMajorVersion" . }}"
spec:
  serviceName: {{ template "elasticsearch.uname" . }}-headless
  selector:
    matchLabels:
        {{/* Only kept for backwards compatible in upgrades, selector cannot be updated */}}
        app: "{{ template "elasticsearch.uname" . }}"
  replicas: {{ $replicas }}
  podManagementPolicy: {{ .Values.podManagementPolicy }}
  updateStrategy:
    type: {{ .Values.updateStrategy }}
  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: data
    {{- with .Values.persistence.annotations  }}
      annotations:
{{ toYaml . | indent 8 }}
    {{- end }}
    spec:
      accessModes:
      {{- range .Values.volumeClaimTemplate.accessModes }}
        - {{ . | quote }}
      {{- end }}
      resources:
        requests:
          {{ $storageSize := include "common.sizing.elasticsearch.storage" . | trim }}
          storage: {{ $storageSize | default .Values.volumeClaimTemplate.resources.requests.storage | quote }}
  {{- if (.Values.global).storageClass }}
      storageClassName: "{{ .Values.global.storageClass }}"
  {{- end }}
  {{- end }}
  template:
    metadata:
      name: "{{ template "elasticsearch.uname" . }}"
      labels:
        {{/* Only kept for backwards compatible in upgrades */}}
        app: "{{ template "elasticsearch.uname" . }}"
        {{- include "elasticsearch.labels.selector" . | nindent 8 }}
        {{- include "elasticsearch.podLabels" . | nindent 8 }}
        app.kubernetes.io/component: {{ .Values.nodeGroup }}
        {{- range $key, $value := .Values.labels }}
        {{ $key }}: {{ $value | quote }}
        {{- end }}
      annotations:
        {{- range $key, $value := .Values.podAnnotations }}
        {{ $key }}: {{ $value | quote }}
        {{- end }}
        {{/* This forces a restart if the configmap has changed */}}
        {{- if .Values.esConfig }}
        configchecksum: {{ include (print .Template.BasePath "/configmap.yaml") . | sha256sum | trunc 63 }}
        {{- end }}
    spec:
      {{- if .Values.schedulerName }}
      schedulerName: "{{ .Values.schedulerName }}"
      {{- end }}
      {{- if .Values.securityContext.enabled }}
      securityContext:
{{ toYaml .Values.podSecurityContext | indent 8 }}
        {{- if .Values.fsGroup }}
        fsGroup: {{ .Values.fsGroup }} # Deprecated value, please use .Values.podSecurityContext.fsGroup
        {{- end }}
      {{- end }}
      {{- if .Values.rbac.create }}
      serviceAccountName: "{{ template "elasticsearch.uname" . }}"
      {{- else if not (eq .Values.rbac.serviceAccountName "") }}
      serviceAccountName: {{ .Values.rbac.serviceAccountName | quote }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 6 }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- $sizingAntiAffinity := include "common.sizing.elasticsearch.antiAffinityConfig" . | trim -}}
      {{- $sizingAntiAffinityTopologyKey := include "common.sizing.elasticsearch.antiAffinityTopologyKeyConfig" . | trim -}}
      {{- $antiAffinity := .Values.antiAffinity | default $sizingAntiAffinity | toString -}}
      {{- $antiAffinity = tpl $antiAffinity . -}}
      {{- $antiAffinityTopologyKey := .Values.antiAffinityTopologyKey | default $sizingAntiAffinityTopologyKey | toString -}}
      {{- $antiAffinityTopologyKey = tpl $antiAffinityTopologyKey . -}}
      {{- if or (eq $antiAffinity "hard") (eq $antiAffinity "soft") .Values.nodeAffinity }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
      {{- end }}
      affinity:
      {{- end }}
      {{- if eq $antiAffinity "hard" }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "{{ template "elasticsearch.uname" .}}"
            topologyKey: {{ $antiAffinityTopologyKey }}
      {{- else if eq $antiAffinity "soft" }}
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: {{ $antiAffinityTopologyKey }}
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "{{ template "elasticsearch.uname" . }}"
      {{- end }}
      {{- with .Values.nodeAffinity }}
        nodeAffinity:
{{ toYaml . | indent 10 }}
      {{- end }}
      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriod }}
      volumes:
        {{- range .Values.secretMounts }}
        - name: {{ .name }}
          secret:
            secretName: {{ .secretName }}
        {{- end }}
        {{- if .Values.esConfig }}
        - name: esconfig
          configMap:
            name: {{ template "elasticsearch.uname" . }}-config
        {{- end }}
        {{- if .Values.createCert }}
        - name: elasticsearch-certs
          secret:
           secretName: {{ template "elasticsearch.uname" . }}-certs
        {{- end }}
{{- if .Values.keystore }}
        - name: keystore
          emptyDir: {}
        {{- range .Values.keystore }}
        - name: keystore-{{ .secretName }}
          secret: {{ toYaml . | nindent 12 }}
        {{- end }}
{{ end }}
      {{- if .Values.extraVolumes }}
{{ tpl .Values.extraVolumes . | indent 8 }}
      {{- end }}
      {{- include "elasticsearch.image.pullSecret.name" (dict "images" (list .Values) "context" $) | nindent 6 }}
      initContainers:
      {{- if .Values.sysctlInitContainer.enabled }}
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "{{ include "elasticsearch.imageRegistry" . }}/{{ .Values.imageRepository }}:{{ .Values.imageTag }}"
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"
        command: ["sysctl", "-w", "vm.max_map_count={{ .Values.sysctlVmMaxMapCount}}"]
        resources:
{{ toYaml .Values.initResources | indent 10 }}
      {{- end }}
{{ if .Values.keystore }}
      - name: keystore
        image: "{{ include "elasticsearch.imageRegistry" . }}/{{ .Values.imageRepository }}:{{ .Values.imageTag }}"
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"
        command:
        - bash
        - -c
        - |
          set -euo pipefail

          elasticsearch-keystore create

          for i in /tmp/keystoreSecrets/*/*; do
            key=$(basename $i)
            echo "Adding file $i to keystore key $key"
            elasticsearch-keystore add-file "$key" "$i"
          done

          # Add the bootstrap password since otherwise the Elasticsearch entrypoint tries to do this on startup
          if [ ! -z ${ELASTIC_PASSWORD+x} ]; then
            echo 'Adding env $ELASTIC_PASSWORD to keystore as key bootstrap.password'
            echo "$ELASTIC_PASSWORD" | elasticsearch-keystore add -x bootstrap.password
          fi

          cp -a /usr/share/elasticsearch/config/elasticsearch.keystore /tmp/keystore/
        env: {{ toYaml .Values.extraEnvs | nindent 10 }}
        resources: {{ toYaml .Values.initResources | nindent 10 }}
        volumeMounts:
          - name: keystore
            mountPath: /tmp/keystore
          {{- range .Values.keystore }}
          - name: keystore-{{ .secretName }}
            mountPath: /tmp/keystoreSecrets/{{ .secretName }}
          {{- end }}
{{ end }}
      {{- if .Values.extraInitContainers }}
{{ tpl .Values.extraInitContainers . | indent 6 }}
      {{- end }}
      containers:
      - name: "{{ template "elasticsearch.name" . }}"
      {{- if .Values.securityContext.enabled }}
        securityContext:
{{ omit .Values.securityContext "enabled" | toYaml | indent 10 }}
      {{- end }}
        image: "{{ include "elasticsearch.imageRegistry" . }}/{{ .Values.imageRepository }}:{{ .Values.imageTag }}"
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"
        readinessProbe:
{{ include "elasticsearch.probe" . | nindent 10 }}
{{ toYaml .Values.readinessProbe | indent 10 }}
        livenessProbe:
{{ include "elasticsearch.probe" . | nindent 10 }}
{{ toYaml .Values.livenessProbe | indent 10 }}
        startupProbe:
{{ include "elasticsearch.probe" . | nindent 10 }}
{{ toYaml .Values.startupProbe | indent 10 }}
        ports:
        - name: http
          containerPort: {{ .Values.httpPort }}
        - name: transport
          containerPort: {{ .Values.transportPort }}
{{- $profileResources := include "common.sizing.elasticsearch.resources" . | trim -}}
{{- $evaluatedResources := .Values.resources }}
{{- if $profileResources }}
{{- $profileResourcesDict := fromYaml $profileResources }}
{{- $evaluatedResources = merge $profileResourcesDict .Values.resources }}
{{- end }}
{{- with $evaluatedResources }}
        resources:
{{ toYaml . | indent 10 }}
{{- end }}
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          {{- if has "master" .Values.roles }}
          - name: cluster.initial_master_nodes
            value: "{{ template "elasticsearch.endpoints" . }}"
          {{- end }}
          {{- if gt (len (include "elasticsearch.roles" .)) 0 }}
          - name: node.roles
            value: "{{ template "elasticsearch.roles" . }}"
          {{- end }}
          {{- if lt (int (include "elasticsearch.esMajorVersion" .)) 7 }}
          - name: discovery.zen.ping.unicast.hosts
            value: "{{ template "elasticsearch.masterService" . }}-headless"
          {{- else }}
          - name: discovery.seed_hosts
            value: "{{ template "elasticsearch.masterService" . }}-headless"
          {{- end }}
          - name: cluster.name
            value: "{{ .Values.clusterName }}"
          - name: network.host
            value: "{{ .Values.networkHost }}"
          {{- if .Values.secret.enabled }}
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: {{ template "elasticsearch.uname" . }}-credentials
                key: password
          {{- end }}
            {{/* Remain compatible with existing configurations, the esJavaOpts is often overriden by users via the small profiles of SUSE Observability.
                 by removing the logging options because they are defined in the docker image now and this settings breaks in the new image version due to different working directory*/}}
          - name: ES_JAVA_OPTS
            value: {{ include "elasticsearch.esJavaOpts" . | replace " -Xlog:disable -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=8,filesize=16m" "" | quote }}
          {{- if .Values.createCert }}
          - name: xpack.security.enabled
            value: "true"
          - name: xpack.security.transport.ssl.enabled
            value: "true"
          - name: xpack.security.http.ssl.enabled
            value: "true"
          - name: xpack.security.transport.ssl.verification_mode
            value: "certificate"
          - name: xpack.security.transport.ssl.key
            value: "/usr/share/elasticsearch/config/certs/tls.key"
          - name: xpack.security.transport.ssl.certificate
            value: "/usr/share/elasticsearch/config/certs/tls.crt"
          - name: xpack.security.transport.ssl.certificate_authorities
            value: "/usr/share/elasticsearch/config/certs/ca.crt"
          - name: xpack.security.http.ssl.key
            value: "/usr/share/elasticsearch/config/certs/tls.key"
          - name: xpack.security.http.ssl.certificate
            value: "/usr/share/elasticsearch/config/certs/tls.crt"
          - name: xpack.security.http.ssl.certificate_authorities
            value: "/usr/share/elasticsearch/config/certs/ca.crt"
          {{- else }}
          - name: xpack.security.enabled
            value: "false"
          {{- end }}
{{- if .Values.extraEnvs }}
{{ toYaml .Values.extraEnvs | indent 10 }}
{{- end }}
        volumeMounts:
          {{- if .Values.persistence.enabled }}
          - name: data
            mountPath: /usr/share/elasticsearch/data
          {{- end }}
          {{- if .Values.createCert }}
          - name: elasticsearch-certs
            mountPath: /usr/share/elasticsearch/config/certs
            readOnly: true
          {{- end }}
{{ if .Values.keystore }}
          - name: keystore
            mountPath: /usr/share/elasticsearch/config/elasticsearch.keystore
            subPath: elasticsearch.keystore
{{ end }}
          {{- range .Values.secretMounts }}
          - name: {{ .name }}
            mountPath: {{ .path }}
            {{- if .subPath }}
            subPath: {{ .subPath }}
            {{- end }}
          {{- end }}
          {{- range $path, $config := .Values.esConfig }}
          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/{{ $path }}
            subPath: {{ $path }}
          {{- end -}}
        {{- if .Values.extraVolumeMounts }}
{{ tpl .Values.extraVolumeMounts . | indent 10 }}
        {{- end }}
      {{- if .Values.masterTerminationFix }}
      {{- if has "master" .Values.roles }}
      # This sidecar will prevent slow master re-election
      # https://github.com/elastic/helm-charts/issues/63
      - name: elasticsearch-master-graceful-termination-handler
        image: "{{ include "elasticsearch.imageRegistry" . }}/{{ .Values.imageRepository }}:{{ .Values.imageTag }}"
        imagePullPolicy: "{{ .Values.imagePullPolicy }}"
        command:
        - "sh"
        - -c
        - |
          #!/usr/bin/env bash
          set -eo pipefail

          http () {
              local path="${1}"
              if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
              else
                BASIC_AUTH=''
              fi
              curl -XGET -s -k --fail ${BASIC_AUTH} {{ .Values.protocol }}://{{ template "elasticsearch.masterService" . }}:{{ .Values.httpPort }}${path}
          }

          cleanup () {
            while true ; do
              local master="$(http "/_cat/master?h=node" || echo "")"
              if [[ $master == "{{ template "elasticsearch.masterService" . }}"* && $master != "${NODE_NAME}" ]]; then
                echo "This node is not master."
                break
              fi
              echo "This node is still master, waiting gracefully for it to step down"
              sleep 1
            done

            exit 0
          }

          trap cleanup SIGTERM

          sleep infinity &
          wait $!
        resources:
{{ toYaml .Values.sidecarResources | indent 10 }}
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        {{- if .Values.extraEnvs }}
{{ toYaml .Values.extraEnvs | indent 10 }}
        {{- end }}
      {{- end }}
      {{- end }}
{{- if .Values.lifecycle }}
        lifecycle:
{{ toYaml .Values.lifecycle | indent 10 }}
{{- end }}
      {{- if .Values.extraContainers }}
{{ tpl .Values.extraContainers . | indent 6 }}
      {{- end }}
