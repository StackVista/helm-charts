{{ template "chart.header" . }}
{{ template "chart.description" . }}

Current chart version is `{{ template "chart.version" . }}`

{{ template "chart.homepageLine" . }}

{{ template "chart.requirementsSection" . }}

## Required Values

In order to successfully install this chart, you **must** provide the following variables:
* `stackstate.license.key`
* `stackstate.baseUrl`

Install them on the command line on Helm with the following command:

```shell
helm install \
--set stackstate.license.key=<your-license-key> \
--set stackstate.baseUrl=<your-base-url> \
stackstate/stackstate
```

## Simplified Sizing Configuration

SUSE Observability now provides built-in sizing profiles that automatically configure all component resources, replica counts, storage sizes, and deployment modes with a single configuration value.

### Quick Start with Sizing Profiles

```yaml
# values.yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"  # Single value configures everything!
    license: "<your-license-key>"
    baseUrl: "<your-base-url>"
    adminPassword: "<bcrypt-hashed-password>"
    receiverApiKey: "<your-receiver-api-key>"
    pullSecret:
      username: "<registry-username>"
      password: "<registry-password>"

# For HA profiles (150-ha, 250-ha, 500-ha, 4000-ha), enable second Victoria Metrics instance
victoria-metrics-1:
  enabled: true

# Disable pull-secret subchart when using global.suseObservability.pullSecret
pull-secret:
  enabled: false
```

```shell
helm install suse-observability . -f values.yaml
```

#### Generating a bcrypt Password Hash

The `adminPassword` must be a bcrypt-hashed password. Generate one using either method:

```shell
# Using htpasswd (commonly available on Linux/macOS)
htpasswd -bnBC 10 "" "your-password" | tr -d ':\n'

# Using Python with bcrypt library
python3 -c "import bcrypt; print(bcrypt.hashpw(b'your-password', bcrypt.gensalt(10)).decode())"

# Using Docker
docker run --rm httpd:alpine htpasswd -bnBC 10 "" "your-password" | tr -d ':\n'
```

#### Understanding receiverApiKey

The `receiverApiKey` is **optional**. It's used to authenticate telemetry data sent to the SUSE Observability receiver endpoints. If not provided:
- The chart will auto-generate a key if `stackstate.receiver.apiKey` is also not set
- You can retrieve the generated key from the Kubernetes secret after installation

Only set this explicitly if you need to:
- Use a specific key for external integrations
- Maintain consistency across reinstallations
- Integrate with pre-configured agents

### Available Sizing Profiles

| Profile | Use Case | HA Mode | Components | VM Instances | Server Split |
|---------|----------|---------|------------|--------------|--------------|
| `trial` | Development/Testing | No | ~10 | 1 | No |
| `10-nonha` | Small non-HA | No | ~10 | 1 | No |
| `20-nonha` | Small non-HA | No | ~20 | 1 | No |
| `50-nonha` | Medium non-HA | No | ~50 | 1 | No |
| `100-nonha` | Large non-HA | No | ~100 | 1 | No |
| `150-ha` | Production HA | Yes | ~150 | 2 | Yes |
| `250-ha` | Production HA | Yes | ~250 | 2 | Yes |
| `500-ha` | Production HA | Yes | ~500 | 2 | Yes |
| `4000-ha` | Enterprise HA | Yes | ~4000 | 2 | Yes |

### What Gets Configured Automatically

A single sizing profile automatically configures:

**Infrastructure Components:**
- ✅ **ClickHouse**: Replicas, CPU/memory resources, storage size
- ✅ **Elasticsearch**: Replicas, CPU/memory resources, storage size
- ✅ **HBase**: Deployment mode (Mono/Distributed), resources for master/regionserver/datanode/namenode/tephra
- ✅ **Kafka**: Replicas, CPU/memory resources, storage size, partition counts
- ✅ **Zookeeper**: Replicas, CPU/memory resources
- ✅ **Victoria Metrics 0**: CPU/memory resources, storage size, retention period
- ✅ **Victoria Metrics 1**: Enablement (HA only), CPU/memory resources, storage size

**SUSE Observability Components:**
- ✅ **API/Server**: Split mode (HA profiles), replica counts, CPU/memory resources
- ✅ **Receiver**: Split mode (base/logs/process-agent for HA), replica counts, resources
- ✅ **Checks, Correlate, State, Sync, Health-Sync**: Replica counts, CPU/memory resources
- ✅ **UI, Notification, Slicing**: Replica counts, CPU/memory resources

**Supporting Services:**
- ✅ **Minio**: CPU/memory resources
- ✅ **KafkaUp Operator**: CPU/memory resources
- ✅ **Prometheus Elasticsearch Exporter**: CPU/memory resources

### Migrating from suse-observability-values Chart

> **⚠️ DEPRECATION NOTICE**
> The `suse-observability-values` chart is deprecated. Use the built-in sizing profiles instead.

**Old workflow (DEPRECATED - Two steps):**

```shell
# Step 1: Generate values file with suse-observability-values chart
helm template suse-observability-values \
  --set sizing.profile=150-ha \
  --set license=<your-license-key> \
  --set baseUrl=<your-base-url> \
  --set pullSecret.username=<username> \
  --set pullSecret.password=<password> \
  suse-observability/suse-observability-values > generated-values.yaml

# Step 2: Install suse-observability chart with generated values
helm install suse-observability . -f generated-values.yaml
```

**New workflow (Recommended - Single step):**

```yaml
# values.yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"  # Replaces the entire values generation step!
    license: "<your-license-key>"
    baseUrl: "<your-base-url>"
    adminPassword: "<bcrypt-hashed-password>"
    receiverApiKey: "<your-receiver-api-key>"
    pullSecret:
      username: "<username>"
      password: "<password>"

victoria-metrics-1:
  enabled: true  # Set to 'false' for non-HA profiles (trial, *-nonha)

pull-secret:
  enabled: false  # Use inlined pull-secret from global.suseObservability
```

```shell
helm install suse-observability . -f values.yaml
```

**Migration checklist:**

1. ✅ Identify your current sizing profile (e.g., `150-ha`)
2. ✅ Create new values file with `global.suseObservability.sizing.profile`
3. ✅ Move credentials to `global.suseObservability.*` section
4. ✅ Set `victoria-metrics-1.enabled: true` for HA profiles (`150-ha`, `250-ha`, `500-ha`, `4000-ha`)
5. ✅ Set `victoria-metrics-1.enabled: false` for non-HA profiles (`trial`, `*-nonha`)
6. ✅ Set `pull-secret.enabled: false` when using `global.suseObservability.pullSecret`
7. ✅ Remove `helm template suse-observability-values` step from deployment scripts
8. ✅ Test installation in non-production environment first

**Benefits of the new approach:**

- ✅ **Simpler**: No separate chart installation needed
- ✅ **Faster**: Single-step installation instead of two steps
- ✅ **Cleaner**: No intermediate generated values files to manage
- ✅ **Maintainable**: Profile updates happen automatically with chart upgrades
- ✅ **Explicit**: Clear profile selection in your values file

### Upgrading Existing Deployments

If you have an existing SUSE Observability installation using the old `suse-observability-values` chart workflow, follow these steps:

**Before upgrading:**

1. **Backup your current values**: Save your existing generated values file and any custom overrides
   ```shell
   kubectl get configmap -n <namespace> -o yaml > backup-configmaps.yaml
   helm get values suse-observability -n <namespace> > current-values.yaml
   ```

2. **Identify your sizing profile**: Check your current `suse-observability-values` configuration to find the profile name (e.g., `150-ha`)

3. **Review resource differences**: The new profiles may have updated resource recommendations. Compare your current resources with the new profile defaults if you have custom overrides

**Upgrade procedure:**

```shell
# 1. Create your new values file with global.suseObservability configuration
#    (see Quick Start example above)

# 2. Perform helm upgrade with the new values
helm upgrade suse-observability suse-observability/suse-observability \
  -n <namespace> \
  -f new-values.yaml

# 3. Verify the upgrade
kubectl get pods -n <namespace>
helm get values suse-observability -n <namespace>
```

**Important considerations:**

- **No downtime expected**: The upgrade is a standard Helm upgrade; pods will be rolled incrementally
- **PVCs are preserved**: Existing persistent volume claims remain intact
- **Secrets are preserved**: Existing secrets (licenses, API keys) are not deleted
- **Rollback available**: Use `helm rollback suse-observability <revision>` if needed

### Why victoria-metrics-1 Must Be Set Manually

The `victoria-metrics-1` subchart enablement cannot be auto-configured by the sizing profile because Helm subchart conditions (`enabled: true/false`) must be evaluated before template rendering. The sizing profile values are applied during template rendering, which is too late to affect subchart inclusion.

**Rule of thumb:**
- HA profiles (`150-ha`, `250-ha`, `500-ha`, `4000-ha`): Set `victoria-metrics-1.enabled: true`
- Non-HA profiles (`trial`, `10-nonha`, `20-nonha`, `50-nonha`, `100-nonha`): Set `victoria-metrics-1.enabled: false` (or omit, as `false` is the default)

### Overriding Sizing Profile Defaults

You can override specific values from the sizing profile when needed:

```yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"

# Override specific component resources
stackstate:
  components:
    api:
      resources:
        requests:
          memory: 16Gi  # Override profile's default of 12Gi

# Override storage sizes
elasticsearch:
  volumeClaimTemplate:
    resources:
      requests:
        storage: 500Gi  # Override profile's default
```

### Global Affinity Configuration

The `global.suseObservability.affinity` section allows you to configure pod scheduling constraints for all components:

```yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"
    affinity:
      # Node affinity - target specific nodes (applies to ALL components)
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/observability
                  operator: Exists

      # Pod affinity - co-locate application pods (does NOT apply to infrastructure)
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/part-of: suse-observability
              topologyKey: kubernetes.io/hostname

      # Pod anti-affinity - spread infrastructure pods (HA profiles only)
      podAntiAffinity:
        # Use hard anti-affinity (pods MUST be on different nodes)
        requiredDuringSchedulingIgnoredDuringExecution: true
        # Spread across nodes (use topology.kubernetes.io/zone for zone spreading)
        topologyKey: "kubernetes.io/hostname"
```

**Affinity scope:**

| Affinity Type | Application Components | Infrastructure Components |
|---------------|------------------------|---------------------------|
| `nodeAffinity` | ✅ Applied | ✅ Applied |
| `podAffinity` | ✅ Applied | ❌ Not applied |
| `podAntiAffinity` | ❌ Not applied | ✅ Applied (HA profiles) |

**Pod anti-affinity modes:**

- `requiredDuringSchedulingIgnoredDuringExecution: true` - **Hard anti-affinity**: Pods will NOT schedule if they cannot be placed on separate nodes. Use when you have enough nodes.
- `requiredDuringSchedulingIgnoredDuringExecution: false` - **Soft anti-affinity**: Pods will prefer separate nodes but can co-locate if necessary. Use when node count is limited.

### Backward Compatibility

The traditional configuration method (without sizing profiles) is still supported for backward compatibility:

```yaml
# Traditional method - still works
stackstate:
  license:
    key: "<your-license-key>"
  baseUrl: "<your-base-url>"
  authentication:
    adminPassword: "<password>"
  components:
    api:
      resources:
        requests:
          cpu: "4000m"
          memory: 12Gi
    # ... manual configuration for each component
```

However, we strongly recommend migrating to sizing profiles for easier maintenance and upgrades.

### Troubleshooting Migration Issues

**Problem: Pods stuck in Pending state after migration**

This usually indicates a scheduling issue, often related to anti-affinity rules:

```shell
# Check pod events
kubectl describe pod <pod-name> -n <namespace>

# If anti-affinity is the issue, use soft anti-affinity
global:
  suseObservability:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution: false  # Soft anti-affinity
```

**Problem: Resources differ from previous installation**

Sizing profiles may have updated resource recommendations. To preserve your previous settings:

```yaml
# Override specific resources while using the profile for everything else
global:
  suseObservability:
    sizing:
      profile: "150-ha"

# Your custom overrides
stackstate:
  components:
    api:
      resources:
        requests:
          memory: 8Gi  # Your previous value
```

**Problem: helm upgrade fails with validation errors**

Ensure you're not mixing old and new configuration styles:

```shell
# Check current values
helm get values suse-observability -n <namespace>

# Common issue: both stackstate.license.key AND global.suseObservability.license set
# Solution: Use only one configuration style
```

**Problem: Pull secret not working**

When using `global.suseObservability.pullSecret`, ensure the pull-secret subchart is disabled:

```yaml
global:
  suseObservability:
    pullSecret:
      username: "user"
      password: "pass"

pull-secret:
  enabled: false  # Required when using global.suseObservability.pullSecret
```

**Problem: victoria-metrics-1 pods not starting (HA profiles)**

Ensure the subchart is explicitly enabled:

```yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"

victoria-metrics-1:
  enabled: true  # Must be set explicitly for HA profiles
```

**Getting help:**

If you encounter issues not covered here:
1. Check pod logs: `kubectl logs <pod-name> -n <namespace>`
2. Review Helm release status: `helm status suse-observability -n <namespace>`
3. Compare rendered templates: `helm template suse-observability . -f values.yaml > rendered.yaml`

{{ template "chart.valuesSection" . }}

## Authentication

For more details on configuring authentication please refer to the [StackState documentation](https://docs.stackstate.com).

### Configuring OpenId connect

Create a `oidc_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values oidc_values.yaml \
  ... \
stackstate/stackstate
```

Example:

```yaml
stackstate:
  authentication:
    oidc:
      clientId: stackstate-client-id
      secret: "some-secret"
      discoveryUri: http://oidc-provider/.well-known/openid-configuration
      authenticationMethod: client_secret_basic
      jwsAlgorithm: RS256
      scope: ["email", "fullname"]
      jwtClaims:
        usernameField: email
        groupsField: groups
      customParameters:
        access_type: offline # Custom request parameter
```

### Configuring Rancher authentication

```
NOTE: for internal use - SUSE ECM development - only. This capability is still under development and not for any other use yet.
```

Rancher authentication uses OpenId Connect (OIDC).

This authentication mechanism in SUSE Observability abstracts away certain OIDC config specific to Rancher to simplify the config required to set up the authentication method between Rancher and SUSE Observability.

To use it create a `rancher_auth_values.yaml` similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values rancher_auth_values.yaml \
  ... \
stackstate/stackstate
```

Example that sets up Rancher authentication.
```yaml
stackstate:
  authentication:
    rancher:
      clientId: "oidc-client"
      secret: "oidc-client-secret"
      baseUrl: "https://rancher-host"
    roles:
      admin: [ "Default Admin" ] # for now, we map here the display names in Rancher to the SO role
```
You can override and extend some of the OIDC config for Rancher with the following fields:
- `discoveryUri`
- `redirectUri`
- `customParams`

If you need to disable TLS verification due to a setup not using verifiable SSL certificates, you can disable SSL checks with some application config (don't use in production):
```yaml
stackstate:
  components:
    server:
      extraEnv:
        open:
          CONFIG_FORCE_stackstate_misc_sslCertificateChecking: false
```

### Configuring Keycloak

Create a `keycloak_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values keycloak_values.yaml \
  ... \
stackstate/stackstate
```

Example:
```yaml
stackstate:
  authentication:
    keycloak:
      url: http://keycloak-server/auth
      realm: test
      clientId: stackstate-client-id
      secret: "some-secret"
      authenticationMethod: client_secret_basic
      jwsAlgorithm: RS256
```

If the defaults of Keycloak don't suit your needs you can extend the yaml to select a different field as username and add groups/roles from fields other than the `roles` in keycloak:
```
stackstate:
  authentication:
    keycloak:
      jwtClaims:
        usernameField: email
        groupsField: groups
```

### Configuring LDAP

To use LDAP create a ldap_values.yaml similar to the example below (update for your LDAP configuration of course). Next to these keys there are 2 optional values that can be set but usually need to be read from a file so you'd specify them on the helm command line (see below):
* `stackstate.authentication.ldap.ssl.trustStore`: The Certificate Truststore to verify server certificates against
* `stackstate.authentication.ldap.ssl.trustCertificates`: The client Certificate trusted by the server (supports PEM, DER and PKCS7 formats)

There are also Base64 Encoded analogues of these values. They are ignored if `trustCertificates` and/or `trustStore` are set:
* `stackstate.authentication.ldap.ssl.trustStoreBase64Encoded`
* `stackstate.authentication.ldap.ssl.trustCertificatesBase64Encoded`
**Note: The reason for `*Base64Encoded` values is this is the only way to upload binary files via KOTS Config**

Only one of `trustCertificates`/`trustCertificatesBase64Encoded` or `trustStore`/`trustStoreBase64Encoded` will be used, `trustCertificates` takes precedence over `trustStore`, and `trustStore`/`trustCertificates` takes precedence over `trustStoreBase64Encoded`/`trustCertificatesBase64Encoded` respectively.

In order to search the groups that a user belongs to and from those get the roles the user can have in StackState we need to config values `rolesKey` and `groupMemberKey`.

Those values in the end will be used to form a filter (and extract the relevant attribute) that looks like:
```({groupMemberKey}=email=admin@sts.com,ou=employees,dc=stackstate,dc=com)```

This returns an entry similar to this:
```dn: {rolesKey}=stackstate-admin,ou=Group,ou=employee,o=stackstate,cn=people,dc=stackstate,dc=com```

Via the {rolesKey} we will get `stackstate-admin` as the role.

Note that the order of the parameters is of importance.

```yaml
stackstate:
  authentication:
    ldap:
      host: ldap-server
      port: 10636 # Standard LDAP SSL port, 10398 for plain LDAP
      bind:
        dn: ou=acme,dc=com # The bind DN to use to authenticate to LDAP
        password: foobar   # The bind password to use to authenticate to LDAP
      ssl:
        type: ssl          # Optional: The SSL Connection type to use to connect to LDAP (Either `ssl` or `starttls`)
      userQuery:
        emailKey: email
        usernameKey: cn
        parameters:
          - ou: employees
          - dc: stackstate
          - dc: com
      groupQuery:
        groupMemberKey: member
        rolesKey: cn
        parameters:
          - ou: groups
          - dc: stackstate
          - dc: com
```

The `trustStore` and `trustCertificates` values need to be set from the command line, as they typically contain binary data. A sample command for this looks like:

```shell
helm install \
--set-file stackstate.authentication.ldap.ssl.trustStore=./ldap-cacerts \
--set-file stackstate.authentication.ldap.ssl.trustCertificates=./ldap-certificate.pem \
--values ldap_values.yaml \
... \
stackstate/stackstate
```

### Configuring file based authentication

If you don't have an external identity provider you can configure users and there roles directly in StackState via a configuration file (a change will result in a restart of the API).

To use this create a `file_auth_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values file_auth_values.yaml \
  ... \
stackstate/stackstate
```

Example that creates 4 different users with the standard roles provided by Stackstate (see our [docs](https://docs.stackstate.com)):
```
stackstate:
  authentication:
    file:
      logins:
        - username: administrator
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-admin]
        - username: guest1
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-guest]
        - username: guest2
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [ stackstate-guest ]
        - username: maintainer
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-power-user, stackstate-guest]
```

## Auto-installing StackPacks
It can be useful to have some StackPacks always installed in StackState. For example the Kuberentes StackPack configured for the cluster running StackState. For that purpose the value `stackstate.stackpacks.installed` can be used to specify the StackPacks that should be installed (by name) and their (required) configuration. As an example here the Kubernetes StackPack will be pre-installed:
```
stackstate:
  stackpacks:
    installed:
      - name: kubernetes
        configuration:
          kubernetes_cluster_name: test
```
