{{ template "chart.header" . }}
{{ template "chart.description" . }}

Current chart version is `{{ template "chart.version" . }}`

{{ template "chart.homepageLine" . }}

{{ template "chart.requirementsSection" . }}

## Required Values

In order to successfully install this chart, you **must** provide the following variables:
* `stackstate.license.key`
* `stackstate.baseUrl`

Install them on the command line on Helm with the following command:

```shell
helm install \
--set stackstate.license.key=<your-license-key> \
--set stackstate.baseUrl=<your-base-url> \
stackstate/stackstate
```

## Simplified Sizing Configuration

SUSE Observability now provides built-in sizing profiles that automatically configure all component resources, replica counts, storage sizes, and deployment modes with a single configuration value.

### Quick Start with Sizing Profiles

```yaml
# values.yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"  # Single value configures everything!
    license: "<your-license-key>"
    baseUrl: "<your-base-url>"
    adminPassword: "<bcrypt-hashed-password>"
    receiverApiKey: "<your-receiver-api-key>"
    pullSecret:
      username: "<registry-username>"
      password: "<registry-password>"

# For HA profiles (150-ha, 250-ha, 500-ha, 4000-ha), enable second Victoria Metrics instance
victoria-metrics-1:
  enabled: true

# Disable pull-secret subchart when using global.suseObservability.pullSecret
pull-secret:
  enabled: false
```

```shell
helm install suse-observability . -f values.yaml
```

### Available Sizing Profiles

| Profile | Use Case | HA Mode | Components | VM Instances | Server Split |
|---------|----------|---------|------------|--------------|--------------|
| `trial` | Development/Testing | No | ~10 | 1 | No |
| `10-nonha` | Small non-HA | No | ~10 | 1 | No |
| `20-nonha` | Small non-HA | No | ~20 | 1 | No |
| `50-nonha` | Medium non-HA | No | ~50 | 1 | No |
| `100-nonha` | Large non-HA | No | ~100 | 1 | No |
| `150-ha` | Production HA | Yes | ~150 | 2 | Yes |
| `250-ha` | Production HA | Yes | ~250 | 2 | Yes |
| `500-ha` | Production HA | Yes | ~500 | 2 | Yes |
| `4000-ha` | Enterprise HA | Yes | ~4000 | 2 | Yes |

### What Gets Configured Automatically

A single sizing profile automatically configures:

**Infrastructure Components:**
- ✅ **ClickHouse**: Replicas, CPU/memory resources, storage size
- ✅ **Elasticsearch**: Replicas, CPU/memory resources, storage size
- ✅ **HBase**: Deployment mode (Mono/Distributed), resources for master/regionserver/datanode/namenode/tephra
- ✅ **Kafka**: Replicas, CPU/memory resources, storage size, partition counts
- ✅ **Zookeeper**: Replicas, CPU/memory resources
- ✅ **Victoria Metrics 0**: CPU/memory resources, storage size, retention period
- ✅ **Victoria Metrics 1**: Enablement (HA only), CPU/memory resources, storage size

**SUSE Observability Components:**
- ✅ **API/Server**: Split mode (HA profiles), replica counts, CPU/memory resources
- ✅ **Receiver**: Split mode (base/logs/process-agent for HA), replica counts, resources
- ✅ **Checks, Correlate, State, Sync, Health-Sync**: Replica counts, CPU/memory resources
- ✅ **UI, Notification, Slicing**: Replica counts, CPU/memory resources

**Supporting Services:**
- ✅ **Minio**: CPU/memory resources
- ✅ **KafkaUp Operator**: CPU/memory resources
- ✅ **Prometheus Elasticsearch Exporter**: CPU/memory resources

### Migrating from suse-observability-values Chart

> **⚠️ DEPRECATION NOTICE**
> The `suse-observability-values` chart is deprecated. Use the built-in sizing profiles instead.

**Old workflow (DEPRECATED - Two steps):**

```shell
# Step 1: Generate values file with suse-observability-values chart
helm template suse-observability-values \
  --set sizing.profile=150-ha \
  --set license=<your-license-key> \
  --set baseUrl=<your-base-url> \
  --set pullSecret.username=<username> \
  --set pullSecret.password=<password> \
  suse-observability/suse-observability-values > generated-values.yaml

# Step 2: Install suse-observability chart with generated values
helm install suse-observability . -f generated-values.yaml
```

**New workflow (Recommended - Single step):**

```yaml
# values.yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"  # Replaces the entire values generation step!
    license: "<your-license-key>"
    baseUrl: "<your-base-url>"
    adminPassword: "<bcrypt-hashed-password>"
    receiverApiKey: "<your-receiver-api-key>"
    pullSecret:
      username: "<username>"
      password: "<password>"

victoria-metrics-1:
  enabled: true  # Set to 'false' for non-HA profiles (trial, *-nonha)

pull-secret:
  enabled: false  # Use inlined pull-secret from global.suseObservability
```

```shell
helm install suse-observability . -f values.yaml
```

**Migration checklist:**

1. ✅ Identify your current sizing profile (e.g., `150-ha`)
2. ✅ Create new values file with `global.suseObservability.sizing.profile`
3. ✅ Move credentials to `global.suseObservability.*` section
4. ✅ Set `victoria-metrics-1.enabled: true` for HA profiles (`150-ha`, `250-ha`, `500-ha`, `4000-ha`)
5. ✅ Set `victoria-metrics-1.enabled: false` for non-HA profiles (`trial`, `*-nonha`)
6. ✅ Set `pull-secret.enabled: false` when using `global.suseObservability.pullSecret`
7. ✅ Remove `helm template suse-observability-values` step from deployment scripts
8. ✅ Test installation in non-production environment first

**Benefits of the new approach:**

- ✅ **Simpler**: No separate chart installation needed
- ✅ **Faster**: Single-step installation instead of two steps
- ✅ **Cleaner**: No intermediate generated values files to manage
- ✅ **Maintainable**: Profile updates happen automatically with chart upgrades
- ✅ **Explicit**: Clear profile selection in your values file

### Overriding Sizing Profile Defaults

You can override specific values from the sizing profile when needed:

```yaml
global:
  suseObservability:
    sizing:
      profile: "150-ha"

# Override specific component resources
stackstate:
  components:
    api:
      resources:
        requests:
          memory: 16Gi  # Override profile's default of 12Gi

# Override storage sizes
elasticsearch:
  volumeClaimTemplate:
    resources:
      requests:
        storage: 500Gi  # Override profile's default
```

### Backward Compatibility

The traditional configuration method (without sizing profiles) is still supported for backward compatibility:

```yaml
# Traditional method - still works
stackstate:
  license:
    key: "<your-license-key>"
  baseUrl: "<your-base-url>"
  authentication:
    adminPassword: "<password>"
  components:
    api:
      resources:
        requests:
          cpu: "4000m"
          memory: 12Gi
    # ... manual configuration for each component
```

However, we strongly recommend migrating to sizing profiles for easier maintenance and upgrades.

{{ template "chart.valuesSection" . }}

## Authentication

For more details on configuring authentication please refer to the [StackState documentation](https://docs.stackstate.com).

### Configuring OpenId connect

Create a `oidc_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values oidc_values.yaml \
  ... \
stackstate/stackstate
```

Example:

```yaml
stackstate:
  authentication:
    oidc:
      clientId: stackstate-client-id
      secret: "some-secret"
      discoveryUri: http://oidc-provider/.well-known/openid-configuration
      authenticationMethod: client_secret_basic
      jwsAlgorithm: RS256
      scope: ["email", "fullname"]
      jwtClaims:
        usernameField: email
        groupsField: groups
      customParameters:
        access_type: offline # Custom request parameter
```

### Configuring Rancher authentication

```
NOTE: for internal use - SUSE ECM development - only. This capability is still under development and not for any other use yet.
```

Rancher authentication uses OpenId Connect (OIDC).

This authentication mechanism in SUSE Observability abstracts away certain OIDC config specific to Rancher to simplify the config required to set up the authentication method between Rancher and SUSE Observability.

To use it create a `rancher_auth_values.yaml` similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values rancher_auth_values.yaml \
  ... \
stackstate/stackstate
```

Example that sets up Rancher authentication.
```yaml
stackstate:
  authentication:
    rancher:
      clientId: "oidc-client"
      secret: "oidc-client-secret"
      baseUrl: "https://rancher-host"
    roles:
      admin: [ "Default Admin" ] # for now, we map here the display names in Rancher to the SO role
```
You can override and extend some of the OIDC config for Rancher with the following fields:
- `discoveryUri`
- `redirectUri`
- `customParams`

If you need to disable TLS verification due to a setup not using verifiable SSL certificates, you can disable SSL checks with some application config (don't use in production):
```yaml
stackstate:
  components:
    server:
      extraEnv:
        open:
          CONFIG_FORCE_stackstate_misc_sslCertificateChecking: false
```

### Configuring Keycloak

Create a `keycloak_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values keycloak_values.yaml \
  ... \
stackstate/stackstate
```

Example:
```yaml
stackstate:
  authentication:
    keycloak:
      url: http://keycloak-server/auth
      realm: test
      clientId: stackstate-client-id
      secret: "some-secret"
      authenticationMethod: client_secret_basic
      jwsAlgorithm: RS256
```

If the defaults of Keycloak don't suit your needs you can extend the yaml to select a different field as username and add groups/roles from fields other than the `roles` in keycloak:
```
stackstate:
  authentication:
    keycloak:
      jwtClaims:
        usernameField: email
        groupsField: groups
```

### Configuring LDAP

To use LDAP create a ldap_values.yaml similar to the example below (update for your LDAP configuration of course). Next to these keys there are 2 optional values that can be set but usually need to be read from a file so you'd specify them on the helm command line (see below):
* `stackstate.authentication.ldap.ssl.trustStore`: The Certificate Truststore to verify server certificates against
* `stackstate.authentication.ldap.ssl.trustCertificates`: The client Certificate trusted by the server (supports PEM, DER and PKCS7 formats)

There are also Base64 Encoded analogues of these values. They are ignored if `trustCertificates` and/or `trustStore` are set:
* `stackstate.authentication.ldap.ssl.trustStoreBase64Encoded`
* `stackstate.authentication.ldap.ssl.trustCertificatesBase64Encoded`
**Note: The reason for `*Base64Encoded` values is this is the only way to upload binary files via KOTS Config**

Only one of `trustCertificates`/`trustCertificatesBase64Encoded` or `trustStore`/`trustStoreBase64Encoded` will be used, `trustCertificates` takes precedence over `trustStore`, and `trustStore`/`trustCertificates` takes precedence over `trustStoreBase64Encoded`/`trustCertificatesBase64Encoded` respectively.

In order to search the groups that a user belongs to and from those get the roles the user can have in StackState we need to config values `rolesKey` and `groupMemberKey`.

Those values in the end will be used to form a filter (and extract the relevant attribute) that looks like:
```({groupMemberKey}=email=admin@sts.com,ou=employees,dc=stackstate,dc=com)```

This returns an entry similar to this:
```dn: {rolesKey}=stackstate-admin,ou=Group,ou=employee,o=stackstate,cn=people,dc=stackstate,dc=com```

Via the {rolesKey} we will get `stackstate-admin` as the role.

Note that the order of the parameters is of importance.

```yaml
stackstate:
  authentication:
    ldap:
      host: ldap-server
      port: 10636 # Standard LDAP SSL port, 10398 for plain LDAP
      bind:
        dn: ou=acme,dc=com # The bind DN to use to authenticate to LDAP
        password: foobar   # The bind password to use to authenticate to LDAP
      ssl:
        type: ssl          # Optional: The SSL Connection type to use to connect to LDAP (Either `ssl` or `starttls`)
      userQuery:
        emailKey: email
        usernameKey: cn
        parameters:
          - ou: employees
          - dc: stackstate
          - dc: com
      groupQuery:
        groupMemberKey: member
        rolesKey: cn
        parameters:
          - ou: groups
          - dc: stackstate
          - dc: com
```

The `trustStore` and `trustCertificates` values need to be set from the command line, as they typically contain binary data. A sample command for this looks like:

```shell
helm install \
--set-file stackstate.authentication.ldap.ssl.trustStore=./ldap-cacerts \
--set-file stackstate.authentication.ldap.ssl.trustCertificates=./ldap-certificate.pem \
--values ldap_values.yaml \
... \
stackstate/stackstate
```

### Configuring file based authentication

If you don't have an external identity provider you can configure users and there roles directly in StackState via a configuration file (a change will result in a restart of the API).

To use this create a `file_auth_values.yaml similar to the example below and add it as an additional argument to the installation:

```
helm install \
  --values file_auth_values.yaml \
  ... \
stackstate/stackstate
```

Example that creates 4 different users with the standard roles provided by Stackstate (see our [docs](https://docs.stackstate.com)):
```
stackstate:
  authentication:
    file:
      logins:
        - username: administrator
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-admin]
        - username: guest1
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-guest]
        - username: guest2
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [ stackstate-guest ]
        - username: maintainer
          passwordHash: 098f6bcd4621d373cade4e832627b4f6
          roles: [stackstate-power-user, stackstate-guest]
```

## Auto-installing StackPacks
It can be useful to have some StackPacks always installed in StackState. For example the Kuberentes StackPack configured for the cluster running StackState. For that purpose the value `stackstate.stackpacks.installed` can be used to specify the StackPacks that should be installed (by name) and their (required) configuration. As an example here the Kubernetes StackPack will be pre-installed:
```
stackstate:
  stackpacks:
    installed:
      - name: kubernetes
        configuration:
          kubernetes_cluster_name: test
```
